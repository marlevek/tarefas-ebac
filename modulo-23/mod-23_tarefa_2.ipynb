{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST\n",
    "\n",
    "É um algoritmo de aprendizado de máquina que combina a saída de várias árvores de decisão para alcançar um único resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PASSO A PASSO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bootstrap + Feature Selection\n",
    "Amostragem aleatória de 'n' exemplos com reposição do conjunto original.\n",
    "2. Seleção de Características\n",
    "Selecionar aleatoriamente um subconjunto de características de tamanho 'max_features'\n",
    "3. Modelagem com Decision Trees:\n",
    "Para cada amostra bootstrap e subconjunto de características, construir uma árvore de decisão\n",
    "4. Agregação\n",
    "Para fazer uma previsão final, agregar as previsões de todas as árvores (por votação majoritária ou média, dependendo do caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIFERENÇA ENTRE BAGGING E RANDOM FOREST\n",
    "\n",
    "Seleção de Amostras: O Bagging utiliza amostragem com reposição para criar subconjuntos do conjunto de dados de treinamento. Isso significa que algumas amostras podem ser selecionadas mais de uma vez para cada subconjunto, enquanto outras podem não ser selecionadas. Já o Random Forest utiliza amostragem sem reposição, onde cada amostra é selecionada apenas uma vez para cada subconjunto.\n",
    "\n",
    "Seleção de Variáveis: No Bagging, todas as variáveis disponíveis são utilizadas na construção de cada árvore de decisão. No Random Forest, um subconjunto aleatório de variáveis é selecionado para cada divisão em cada árvore de decisão. Esse processo, conhecido como seleção aleatória de variáveis, contribui para reduzir a correlação entre as árvores e aumentar a diversidade do conjunto final.\n",
    "\n",
    "Interpretação: O Bagging geralmente é mais fácil de interpretar do que o Random Forest, pois as árvores de decisão individuais são construídas com base em todo o conjunto de dados. Já no Random Forest, a seleção aleatória de variáveis torna a interpretação individual das árvores mais complexa, mas contribui para a robustez geral do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementar em python o Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "def feature_selection(X, n_features):\n",
    "    features = np.random.choice(X.shape[1], n_features, replace=False)\n",
    "    return features\n",
    "\n",
    "# Modelagem com Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class SimpleRandomForest:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.features = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = bootstrap_sample(X, y)\n",
    "            if self.max_features == 'sqrt':\n",
    "                n_features = int(np.sqrt(X.shape[1]))\n",
    "            elif self.max_features == 'log2':\n",
    "                n_features = int(np.log2(X.shape[1]))\n",
    "            else:\n",
    "                n_features = X.shape[1]\n",
    "            \n",
    "            features = feature_selection(X, n_features)\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X_sample[:, features], y_sample)\n",
    "            self.trees.append(tree)\n",
    "            self.features.append(features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X[:, features]) for tree, features in zip(self.trees, self.features)])\n",
    "        return np.swapaxes(tree_preds, 0, 1).mode(axis=1)[0]\n",
    "\n",
    "\n",
    "# Agregação\n",
    "from scipy.stats import mode\n",
    "\n",
    "def predict(self, X):\n",
    "    tree_predict = np.array([tree.predict(X[:, features]) for tree, features in zip(self.trees, self.features)])\n",
    "    return mode(tree_predict, axis=0)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregar dados\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Treinar Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Prever e avaliar\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Avaliação\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
