{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cite 5 diferenças entre o AdaBoost e o GBM\n",
    "\n",
    "1. Mecanismo de Atualização dos Pesos:\n",
    "\n",
    "AdaBoosting: Ajusta os pesos das instâncias do conjunto de treinamento com base no desempenho do modelo anterior. A cada iteração, as instâncias mal classificadas recebem um peso maior, incentivando o próximo modelo a se concentrar mais nesses exemplos difíceis.\n",
    "\n",
    "GBM: Utiliza uma abordagem de gradiente descendente para minimizar a função de perda. Em vez de ajustar os pesos das instâncias, o GBM ajusta os valores preditos diretamente, somando novos modelos que corrigem os erros residuais dos modelos anteriores.\n",
    "\n",
    "2. Função de Perda:\n",
    "\n",
    "AdaBoosting: Originalmente, AdaBoost foi desenvolvido para minimizar o erro de classificação binária, mas variantes podem ser usadas para regressão. Utiliza uma função de perda exponencial para ajustar os pesos.\n",
    "\n",
    "GBM: Pode usar várias funções de perda, como erro quadrático (para regressão), entropia cruzada (para classificação) e muitas outras, tornando-o mais flexível em termos de tipos de problemas que pode abordar.\n",
    "\n",
    "3. Tipo de Modelos de Base:\n",
    "\n",
    "AdaBoosting: Comumente utiliza árvores de decisão de profundidade 1 (stumps) como modelos de base, mas pode usar outros modelos simples.\n",
    "\n",
    "GBM: Geralmente usa árvores de decisão de maior profundidade (não limitadas a stumps) como modelos de base, o que permite capturar interações mais complexas entre as características.\n",
    "\n",
    "4. Construção Sequencial:\n",
    "\n",
    "AdaBoosting: Os modelos são treinados sequencialmente, e cada modelo novo tenta corrigir os erros dos modelos anteriores aumentando os pesos das instâncias mal classificadas.\n",
    "\n",
    "GBM: Cada modelo novo é ajustado para minimizar os erros residuais dos modelos anteriores usando um procedimento de gradiente descendente, ajustando a função de perda de forma direta.\n",
    "\n",
    "5. Robustez a Ruídos e Outliers:\n",
    "\n",
    "AdaBoosting: Pode ser sensível a ruídos e outliers no conjunto de dados, pois aumenta os pesos das instâncias mal classificadas, o que pode incluir outliers.\n",
    "\n",
    "GBM: Tende a ser mais robusto a ruídos e outliers, pois o ajuste é feito para minimizar uma função de perda escolhida, que pode ser menos afetada por outliers dependendo da função de perda utilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acesse o link Scikit-learn - GBM, leia a explicação (traduza se preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840234741105356"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Regressão\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ")\n",
    "est = est.fit(X_train, y_train)  # fit with 100 trees\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\n",
    "_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classificação\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cite 5 Hyperparâmetros importantes do GBM\n",
    "\n",
    "* Número de Árvores (n_estimators): Define o número de árvores que serão construídas pelo GBM. Um valor maior geralmente resulta em um modelo mais robusto, mas pode aumentar o tempo de treinamento e o risco de overfitting.\n",
    "* Taxa de Aprendizado (learning_rate): Controla a contribuição de cada árvore no processo de aprendizado. Reduzir a taxa de aprendizado geralmente requer aumentar o número de árvores para manter o desempenho do modelo, mas pode melhorar a generalização.\n",
    "* Profundidade Máxima das Árvores (max_depth): Especifica a profundidade máxima das árvores de decisão individuais no GBM. Árvores mais profundas podem capturar interações mais complexas nos dados de treinamento, mas aumentam o risco de overfitting.\n",
    "* Número Mínimo de Amostras em Folhas (min_samples_leaf): Define o número mínimo de amostras necessárias em uma folha para continuar dividindo um nó. Isso ajuda a controlar o tamanho das árvores e, consequentemente, a complexidade do modelo.\n",
    "* Subamostragem de Características (subsample): Especifica a fração de amostras a serem usadas para treinar cada árvore. Valores menores resultam em uma maior variância, mas também podem ajudar a reduzir o overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'learning_rate': 0.01, 'max_depth': 1, 'min_samples_leaf': 0.01, 'n_estimators': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Carregar Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Definindo a grade de hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'min_samples_leaf': [0.01, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Criando o modelo\n",
    "gbm = GradientBoostingRegressor(random_state=5)\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Executando o GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores hiperparâmetros:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "A maior diferença entre o Gradient Boosting Machine (GBM) padrão e o Stochastic Gradient Boosting (SGB) está na forma como são construídos os conjuntos de dados usados para treinar cada árvore individual no ensemble.\n",
    "\n",
    "* No GBM padrão:\n",
    "\n",
    "Cada árvore é treinada usando o conjunto de dados completo.\n",
    "A cada iteração, todas as amostras são utilizadas para construir a árvore, e a atualização é feita para reduzir o erro global.\n",
    "\n",
    "* No Stochastic Gradient Boosting (SGB):\n",
    "\n",
    "Cada árvore é treinada em uma subamostra aleatória do conjunto de dados completo.\n",
    "A subamostra é geralmente criada utilizando uma abordagem de amostragem aleatória com reposição (bootstrap), onde cada árvore vê apenas uma fração dos dados, selecionada aleatoriamente.\n",
    "Esta técnica introduz uma aleatoriedade controlada no processo de treinamento, o que pode ajudar a reduzir o overfitting e melhorar a generalização do modelo.\n",
    "\n",
    "A principal diferença entre GBM padrão e Stochastic GBM (SGB) reside na introdução de aleatoriedade no SGB através do uso de subamostras para treinar cada árvore, enquanto o GBM padrão utiliza o conjunto de dados completo em cada iteração."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
